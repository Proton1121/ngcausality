{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WfZBAAJ4tcj"
      },
      "source": [
        "# cMamba; Var(2); T = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhm6IGfJ445r",
        "outputId": "ae7c3e6f-8601-48cb-a9da-e5ae3ed7f5d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2ECVf6_5IV3",
        "outputId": "a9794e49-956e-484c-b21b-f6108909c8d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ngcausality'...\n",
            "remote: Enumerating objects: 439, done.\u001b[K\n",
            "remote: Counting objects: 100% (254/254), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "remote: Total 439 (delta 177), reused 201 (delta 143), pack-reused 185 (from 1)\u001b[K\n",
            "Receiving objects: 100% (439/439), 2.88 MiB | 6.36 MiB/s, done.\n",
            "Resolving deltas: 100% (253/253), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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@github.com/Proton1121/ngcausality.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avf5RXuU50ck",
        "outputId": "2cb0640c-a090-466b-d346-c8aa598fe9cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ngcausality\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ngcausality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FBbOyQ-4tcm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from data.synthetic import simulate_lorenz_96, simulate_var\n",
        "from data.dream import generate_causal_matrix\n",
        "from models.cmamba import cMamba, train_model_ista, MambaConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEbcASByB-7M"
      },
      "outputs": [],
      "source": [
        "save_dir = '/content/drive/MyDrive/ngcausality_results/' + 'cmamba_var2_t1000/'\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkQyyD7B4tco"
      },
      "outputs": [],
      "source": [
        "# For GPU acceleration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFaFwK384tcp"
      },
      "outputs": [],
      "source": [
        "# Simulate data\n",
        "X_np, beta, GC = simulate_var(p=20, T=1000, lag=2)\n",
        "X = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqL4DxxjEe84"
      },
      "outputs": [],
      "source": [
        "# Save the simulated data to Google Drive\n",
        "np.save(os.path.join(save_dir, 'X_np.npy'), X_np)  # Save X_np (simulated data)\n",
        "np.save(os.path.join(save_dir, 'GC.npy'), GC)  # Save GC (Granger causality matrix)\n",
        "\n",
        "torch.save(X, os.path.join(save_dir, 'X_tensor.pt'))\n",
        "\n",
        "with open(os.path.join(save_dir, 'data_shapes.txt'), 'w') as f:\n",
        "    f.write(f'Shape of X_np: {X_np.shape}\\n')\n",
        "    f.write(f'Shape of GC: {GC.shape}\\n')\n",
        "    f.write(f'Shape of X (torch tensor): {X.shape}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3SH5MCp4tcr"
      },
      "outputs": [],
      "source": [
        "# Plot data\n",
        "fig, axarr = plt.subplots(1, 2, figsize=(16, 5))\n",
        "axarr[0].plot(X_np)\n",
        "axarr[0].set_xlabel('T')\n",
        "axarr[0].set_title('Entire time series')\n",
        "axarr[1].plot(X_np[:50, :5])\n",
        "axarr[1].set_xlabel('T')\n",
        "axarr[1].set_title('First 50 time points for the first 5 variables')\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "# Step 5: Save the plot to Google Drive\n",
        "plot_filename = os.path.join(save_dir, 'data_plots.png')\n",
        "plt.savefig(plot_filename)  # Save the plot as a PNG file in Google Drive\n",
        "\n",
        "# Optionally, close the plot to prevent it from displaying in the notebook (you can skip this if you want to see it in the notebook)\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "id": "njNLOBz74tcs",
        "outputId": "694371ff-3387-4c44-fe1c-af43dbf55f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------Iter = 50----------\n",
            "Loss = 0.563054\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 100----------\n",
            "Loss = 0.556327\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 150----------\n",
            "Loss = 0.550440\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 200----------\n",
            "Loss = 0.544663\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 250----------\n",
            "Loss = 0.538901\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 300----------\n",
            "Loss = 0.533140\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 350----------\n",
            "Loss = 0.527380\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 400----------\n",
            "Loss = 0.521620\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 450----------\n",
            "Loss = 0.515859\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 500----------\n",
            "Loss = 0.510099\n",
            "Variable usage = 100.00%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b4cf1e76cc90>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# Train with ISTA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   train_loss_list=train_model_ista(cmamba, X, context=10, lr=1e-2, max_iter=30000, lam=(0.016 + 0.008 * (1+i)), lam_ridge=1e-3,\n\u001b[0m\u001b[1;32m     14\u001b[0m                    check_every=50)\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ngcausality/models/cmamba.py\u001b[0m in \u001b[0;36mtrain_model_ista\u001b[0;34m(cmamba, X, context, lr, max_iter, lam, lam_ridge, lookback, check_every, verbose)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0msmooth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcmamba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# Take prox step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i in range(20):\n",
        "  save_dir = '/content/drive/MyDrive/ngcausality_results/' + 'cmamba_var2_t1000/' + str(0.016 + 0.008 * (1+i)) + '/'\n",
        "\n",
        "  if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "  #cMamba config\n",
        "  cmamba_config = MambaConfig(d_model = 20, dt_rank = 3, bias = True)\n",
        "  #Set up model\n",
        "  cmamba = cMamba(cmamba_config).to(device=device)\n",
        "\n",
        "  # Train with ISTA\n",
        "  train_loss_list=train_model_ista(cmamba, X, context=10, lr=1e-2, max_iter=30000, lam=(0.016 + 0.008 * (1+i)), lam_ridge=1e-3,\n",
        "                   check_every=50)\n",
        "\n",
        "  # Loss function plot\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  train_loss_np = [loss.cpu().detach().numpy() for loss in train_loss_list]\n",
        "  plt.plot(50 * np.arange(len(train_loss_np)), train_loss_np)\n",
        "  plt.title('cMamba training')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Training steps')\n",
        "  plt.tight_layout()\n",
        "  loss_plot_path = os.path.join(save_dir, f'loss_plot_{0.016 + 0.008 * (1+i)}.png')\n",
        "  plt.savefig(loss_plot_path)  # Save the loss plot to Google Drive\n",
        "  plt.close()  # Close the plot to prevent it from displaying\n",
        "\n",
        "  # Verify learned Granger causality\n",
        "  GC_est = cmamba.GC().cpu().data.numpy()\n",
        "\n",
        "  results_file_path = os.path.join(save_dir, f'gc_results_{0.016 + 0.008 * (1+i)}.txt')\n",
        "  with open(results_file_path, 'w') as f:\n",
        "    f.write(f'True variable usage = {100 * np.mean(GC)}%\\n')\n",
        "    f.write(f'Estimated variable usage = {100 * np.mean(GC_est)}%\\n')\n",
        "    f.write(f'Accuracy = {100 * np.mean(GC == GC_est)}%\\n')\n",
        "    f.write(f'True positives = {np.sum((GC == 1) & (GC_est == 1))}\\n')\n",
        "    f.write(f'True negatives = {np.sum((GC == 0) & (GC_est == 0))}\\n')\n",
        "    f.write(f'False positives = {np.sum((GC == 0) & (GC_est == 1))}\\n')\n",
        "    f.write(f'False negatives = {np.sum((GC == 1) & (GC_est == 0))}\\n')\n",
        "\n",
        "  # Make figures for Granger causality matrices\n",
        "  fig, axarr = plt.subplots(1, 2, figsize=(16, 5))\n",
        "  axarr[0].imshow(GC, cmap='Blues')\n",
        "  axarr[0].set_title('GC actual')\n",
        "  axarr[0].set_ylabel('Affected series')\n",
        "  axarr[0].set_xlabel('Causal series')\n",
        "  axarr[0].set_xticks([])\n",
        "  axarr[0].set_yticks([])\n",
        "\n",
        "  axarr[1].imshow(GC_est, cmap='Blues', vmin=0, vmax=1, extent=(0, len(GC_est), len(GC_est), 0))\n",
        "  axarr[1].set_title('GC estimated')\n",
        "  axarr[1].set_ylabel('Affected series')\n",
        "  axarr[1].set_xlabel('Causal series')\n",
        "  axarr[1].set_xticks([])\n",
        "  axarr[1].set_yticks([])\n",
        "\n",
        "  # Mark disagreements\n",
        "  for i in range(len(GC_est)):\n",
        "    for j in range(len(GC_est)):\n",
        "        if GC[i, j] != GC_est[i, j]:\n",
        "            rect = plt.Rectangle((j, i-0.05), 1, 1, facecolor='none', edgecolor='red', linewidth=1)\n",
        "            axarr[1].add_patch(rect)\n",
        "\n",
        "  gc_plot_path = os.path.join(save_dir, f'gc_plot_{0.016 + 0.008 * (1+i)}.png')\n",
        "  plt.savefig(gc_plot_path)  # Save the GC plot to Google Drive\n",
        "  plt.close()  # Close the plot to prevent it from displaying"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}