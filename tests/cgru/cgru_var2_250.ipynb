{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WfZBAAJ4tcj"
      },
      "source": [
        "# cGRU; Var(2); T = 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhm6IGfJ445r",
        "outputId": "d7be93bd-fb6f-4d29-9b99-6fc3285a6c54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2ECVf6_5IV3",
        "outputId": "4459926d-c1a0-4214-daee-9af3a06ca1dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ngcausality'...\n",
            "remote: Enumerating objects: 261, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 261 (delta 54), reused 8 (delta 8), pack-reused 185 (from 1)\u001b[K\n",
            "Receiving objects: 100% (261/261), 2.80 MiB | 22.92 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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@github.com/Proton1121/ngcausality.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avf5RXuU50ck",
        "outputId": "93928f69-1a2c-42d2-ad13-538661ee92c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ngcausality\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ngcausality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FBbOyQ-4tcm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from data.synthetic import simulate_lorenz_96, simulate_var\n",
        "from data.dream import generate_causal_matrix\n",
        "from models.cgru import cGRU, train_model_ista"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEbcASByB-7M"
      },
      "outputs": [],
      "source": [
        "save_dir = '/content/drive/MyDrive/ngcausality_results/' + 'cgru_var2_t250/'\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkQyyD7B4tco"
      },
      "outputs": [],
      "source": [
        "# For GPU acceleration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFaFwK384tcp"
      },
      "outputs": [],
      "source": [
        "# Simulate data\n",
        "X_np, beta, GC = simulate_var(p=20, T=250, lag=2)\n",
        "X = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqL4DxxjEe84"
      },
      "outputs": [],
      "source": [
        "# Save the simulated data to Google Drive\n",
        "np.save(os.path.join(save_dir, 'X_np.npy'), X_np)  # Save X_np (simulated data)\n",
        "np.save(os.path.join(save_dir, 'GC.npy'), GC)  # Save GC (Granger causality matrix)\n",
        "\n",
        "torch.save(X, os.path.join(save_dir, 'X_tensor.pt'))\n",
        "\n",
        "with open(os.path.join(save_dir, 'data_shapes.txt'), 'w') as f:\n",
        "    f.write(f'Shape of X_np: {X_np.shape}\\n')\n",
        "    f.write(f'Shape of GC: {GC.shape}\\n')\n",
        "    f.write(f'Shape of X (torch tensor): {X.shape}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3SH5MCp4tcr"
      },
      "outputs": [],
      "source": [
        "# Plot data\n",
        "fig, axarr = plt.subplots(1, 2, figsize=(16, 5))\n",
        "axarr[0].plot(X_np)\n",
        "axarr[0].set_xlabel('T')\n",
        "axarr[0].set_title('Entire time series')\n",
        "axarr[1].plot(X_np[:50, :5])\n",
        "axarr[1].set_xlabel('T')\n",
        "axarr[1].set_title('First 50 time points for the first 5 variables')\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "# Step 5: Save the plot to Google Drive\n",
        "plot_filename = os.path.join(save_dir, 'data_plots.png')\n",
        "plt.savefig(plot_filename)  # Save the plot as a PNG file in Google Drive\n",
        "\n",
        "# Optionally, close the plot to prevent it from displaying in the notebook (you can skip this if you want to see it in the notebook)\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "njNLOBz74tcs",
        "outputId": "eecf0839-924e-4235-c862-490ee774e0c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------Iter = 50----------\n",
            "Loss = 0.840331\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 100----------\n",
            "Loss = 0.690981\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 150----------\n",
            "Loss = 0.568917\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 200----------\n",
            "Loss = 0.469083\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 250----------\n",
            "Loss = 0.387400\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 300----------\n",
            "Loss = 0.320555\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 350----------\n",
            "Loss = 0.265845\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 400----------\n",
            "Loss = 0.221064\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 450----------\n",
            "Loss = 0.184408\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 500----------\n",
            "Loss = 0.154400\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 550----------\n",
            "Loss = 0.129834\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 600----------\n",
            "Loss = 0.109721\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 650----------\n",
            "Loss = 0.093254\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 700----------\n",
            "Loss = 0.079769\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 750----------\n",
            "Loss = 0.068727\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 800----------\n",
            "Loss = 0.059684\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 850----------\n",
            "Loss = 0.052277\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 900----------\n",
            "Loss = 0.046209\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 950----------\n",
            "Loss = 0.041237\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1000----------\n",
            "Loss = 0.037163\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1050----------\n",
            "Loss = 0.033823\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1100----------\n",
            "Loss = 0.031085\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1150----------\n",
            "Loss = 0.028838\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1200----------\n",
            "Loss = 0.026995\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1250----------\n",
            "Loss = 0.025482\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1300----------\n",
            "Loss = 0.024238\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1350----------\n",
            "Loss = 0.023216\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1400----------\n",
            "Loss = 0.022375\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1450----------\n",
            "Loss = 0.021682\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1500----------\n",
            "Loss = 0.021110\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1550----------\n",
            "Loss = 0.020638\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1600----------\n",
            "Loss = 0.020248\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1650----------\n",
            "Loss = 0.019924\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1700----------\n",
            "Loss = 0.019654\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1750----------\n",
            "Loss = 0.019430\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1800----------\n",
            "Loss = 0.019242\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1850----------\n",
            "Loss = 0.019084\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1900----------\n",
            "Loss = 0.018950\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1950----------\n",
            "Loss = 0.018837\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2000----------\n",
            "Loss = 0.018741\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2050----------\n",
            "Loss = 0.018658\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2100----------\n",
            "Loss = 0.018586\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2150----------\n",
            "Loss = 0.018523\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2200----------\n",
            "Loss = 0.018468\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2250----------\n",
            "Loss = 0.018419\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2300----------\n",
            "Loss = 0.018376\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2350----------\n",
            "Loss = 0.018336\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2400----------\n",
            "Loss = 0.018300\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2450----------\n",
            "Loss = 0.018266\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2500----------\n",
            "Loss = 0.018236\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2550----------\n",
            "Loss = 0.018206\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2600----------\n",
            "Loss = 0.018179\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2650----------\n",
            "Loss = 0.018153\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2700----------\n",
            "Loss = 0.018128\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2750----------\n",
            "Loss = 0.018104\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2800----------\n",
            "Loss = 0.018081\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2850----------\n",
            "Loss = 0.018058\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2900----------\n",
            "Loss = 0.018037\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2950----------\n",
            "Loss = 0.018015\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3000----------\n",
            "Loss = 0.017994\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3050----------\n",
            "Loss = 0.017973\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3100----------\n",
            "Loss = 0.017953\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3150----------\n",
            "Loss = 0.017933\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3200----------\n",
            "Loss = 0.017913\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3250----------\n",
            "Loss = 0.017893\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3300----------\n",
            "Loss = 0.017874\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3350----------\n",
            "Loss = 0.017854\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3400----------\n",
            "Loss = 0.017835\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3450----------\n",
            "Loss = 0.017816\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3500----------\n",
            "Loss = 0.017797\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3550----------\n",
            "Loss = 0.017778\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3600----------\n",
            "Loss = 0.017760\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3650----------\n",
            "Loss = 0.017741\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3700----------\n",
            "Loss = 0.017723\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3750----------\n",
            "Loss = 0.017704\n",
            "Variable usage = 100.00%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c08cee6150b7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Train with ISTA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   train_loss_list = train_model_ista(cgru, X, context=10, lr=1e-1, max_iter=20000, lam=(0.0004 * (1+i)), lam_ridge=1e-2,\n\u001b[0m\u001b[1;32m     12\u001b[0m                    check_every=50)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ngcausality/models/cgru.py\u001b[0m in \u001b[0;36mtrain_model_ista\u001b[0;34m(cgru, X, context, lr, max_iter, lam, lam_ridge, lookback, check_every, verbose)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcgru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                 \u001b[0mprox_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mcgru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ngcausality/models/cgru.py\u001b[0m in \u001b[0;36mprox_update\u001b[0;34m(network, lam, lr)\u001b[0m\n\u001b[1;32m    101\u001b[0m     W.data = ((W / torch.clamp(norm, min=(lam * lr)))\n\u001b[1;32m    102\u001b[0m               * torch.clamp(norm - (lr * lam), min=0.0))\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mflatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_acceptable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             ):\n\u001b[1;32m    249\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/backends/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i in range(20):\n",
        "  save_dir = '/content/drive/MyDrive/ngcausality_results/' + 'cgru_var2_t250/' + str(0.0004 * (1+i)) + '/'\n",
        "\n",
        "  if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "  #Set up model\n",
        "  cgru = cGRU(X.shape[-1], hidden=100).to(device=device)\n",
        "\n",
        "  # Train with ISTA\n",
        "  train_loss_list = train_model_ista(cgru, X, context=10, lr=1e-1, max_iter=20000, lam=(0.0004 * (1+i)), lam_ridge=1e-2,\n",
        "                   check_every=50)\n",
        "\n",
        "  # Loss function plot\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  train_loss_np = [loss.cpu().detach().numpy() for loss in train_loss_list]\n",
        "  plt.plot(50 * np.arange(len(train_loss_np)), train_loss_np)\n",
        "  plt.title('cGRU training')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Training steps')\n",
        "  plt.tight_layout()\n",
        "  loss_plot_path = os.path.join(save_dir, f'loss_plot_{0.0004 * (1+i)}.png')\n",
        "  plt.savefig(loss_plot_path)  # Save the loss plot to Google Drive\n",
        "  plt.close()  # Close the plot to prevent it from displaying\n",
        "\n",
        "  # Verify learned Granger causality\n",
        "  GC_est = cgru.GC().cpu().data.numpy()\n",
        "\n",
        "  results_file_path = os.path.join(save_dir, f'gc_results_{0.0004 * (1+i)}.txt')\n",
        "  with open(results_file_path, 'w') as f:\n",
        "    f.write(f'True variable usage = {100 * np.mean(GC)}%\\n')\n",
        "    f.write(f'Estimated variable usage = {100 * np.mean(GC_est)}%\\n')\n",
        "    f.write(f'Accuracy = {100 * np.mean(GC == GC_est)}%\\n')\n",
        "    f.write(f'True positives = {np.sum((GC == 1) & (GC_est == 1))}\\n')\n",
        "    f.write(f'True negatives = {np.sum((GC == 0) & (GC_est == 0))}\\n')\n",
        "    f.write(f'False positives = {np.sum((GC == 0) & (GC_est == 1))}\\n')\n",
        "    f.write(f'False negatives = {np.sum((GC == 1) & (GC_est == 0))}\\n')\n",
        "\n",
        "  # Make figures for Granger causality matrices\n",
        "  fig, axarr = plt.subplots(1, 2, figsize=(16, 5))\n",
        "  axarr[0].imshow(GC, cmap='Blues')\n",
        "  axarr[0].set_title('GC actual')\n",
        "  axarr[0].set_ylabel('Affected series')\n",
        "  axarr[0].set_xlabel('Causal series')\n",
        "  axarr[0].set_xticks([])\n",
        "  axarr[0].set_yticks([])\n",
        "\n",
        "  axarr[1].imshow(GC_est, cmap='Blues', vmin=0, vmax=1, extent=(0, len(GC_est), len(GC_est), 0))\n",
        "  axarr[1].set_title('GC estimated')\n",
        "  axarr[1].set_ylabel('Affected series')\n",
        "  axarr[1].set_xlabel('Causal series')\n",
        "  axarr[1].set_xticks([])\n",
        "  axarr[1].set_yticks([])\n",
        "\n",
        "  # Mark disagreements\n",
        "  for i in range(len(GC_est)):\n",
        "    for j in range(len(GC_est)):\n",
        "        if GC[i, j] != GC_est[i, j]:\n",
        "            rect = plt.Rectangle((j, i-0.05), 1, 1, facecolor='none', edgecolor='red', linewidth=1)\n",
        "            axarr[1].add_patch(rect)\n",
        "\n",
        "  gc_plot_path = os.path.join(save_dir, f'gc_plot_{0.0004 * (1+i)}.png')\n",
        "  plt.savefig(gc_plot_path)  # Save the GC plot to Google Drive\n",
        "  plt.close()  # Close the plot to prevent it from displaying"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}