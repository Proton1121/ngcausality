{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WfZBAAJ4tcj"
      },
      "source": [
        "# cDLinear; Var(2); T = 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhm6IGfJ445r",
        "outputId": "4979c7a2-ee43-45c1-8a31-25447f1dcaa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2ECVf6_5IV3",
        "outputId": "a9d4d977-63f5-45d8-89f7-7810fbb5573a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ngcausality'...\n",
            "remote: Enumerating objects: 215, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 215 (delta 21), reused 8 (delta 8), pack-reused 185 (from 1)\u001b[K\n",
            "Receiving objects: 100% (215/215), 2.73 MiB | 6.76 MiB/s, done.\n",
            "Resolving deltas: 100% (97/97), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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@github.com/Proton1121/ngcausality.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avf5RXuU50ck",
        "outputId": "63e8cccf-515d-4c30-88da-e7fe0f083522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ngcausality\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ngcausality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FBbOyQ-4tcm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from data.synthetic import simulate_lorenz_96, simulate_var\n",
        "from data.dream import generate_causal_matrix\n",
        "from models.cdlinear import cDLinear, train_model_ista"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEbcASByB-7M"
      },
      "outputs": [],
      "source": [
        "save_dir = '/content/drive/MyDrive/ngcausality_results/' + 'cdlinear_var2_t250/'\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkQyyD7B4tco"
      },
      "outputs": [],
      "source": [
        "# For GPU acceleration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFaFwK384tcp"
      },
      "outputs": [],
      "source": [
        "# Simulate data\n",
        "X_np, beta, GC = simulate_var(p=20, T=250, lag=2)\n",
        "X = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqL4DxxjEe84"
      },
      "outputs": [],
      "source": [
        "# Save the simulated data to Google Drive\n",
        "np.save(os.path.join(save_dir, 'X_np.npy'), X_np)  # Save X_np (simulated data)\n",
        "np.save(os.path.join(save_dir, 'GC.npy'), GC)  # Save GC (Granger causality matrix)\n",
        "\n",
        "torch.save(X, os.path.join(save_dir, 'X_tensor.pt'))\n",
        "\n",
        "with open(os.path.join(save_dir, 'data_shapes.txt'), 'w') as f:\n",
        "    f.write(f'Shape of X_np: {X_np.shape}\\n')\n",
        "    f.write(f'Shape of GC: {GC.shape}\\n')\n",
        "    f.write(f'Shape of X (torch tensor): {X.shape}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3SH5MCp4tcr"
      },
      "outputs": [],
      "source": [
        "# Plot data\n",
        "fig, axarr = plt.subplots(1, 2, figsize=(16, 5))\n",
        "axarr[0].plot(X_np)\n",
        "axarr[0].set_xlabel('T')\n",
        "axarr[0].set_title('Entire time series')\n",
        "axarr[1].plot(X_np[:50])\n",
        "axarr[1].set_xlabel('T')\n",
        "axarr[1].set_title('First 50 time points')\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "# Step 5: Save the plot to Google Drive\n",
        "plot_filename = os.path.join(save_dir, 'data_plots.png')\n",
        "plt.savefig(plot_filename)  # Save the plot as a PNG file in Google Drive\n",
        "\n",
        "# Optionally, close the plot to prevent it from displaying in the notebook (you can skip this if you want to see it in the notebook)\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "njNLOBz74tcs",
        "outputId": "9f3f537d-2e40-4b32-cffc-fa5f9ceb9629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------Iter = 100----------\n",
            "Loss = 0.022667\n",
            "nonsmooth = 0.008752\n",
            "smooth = 0.013915\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 200----------\n",
            "Loss = 0.021972\n",
            "nonsmooth = 0.008751\n",
            "smooth = 0.013221\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 300----------\n",
            "Loss = 0.021435\n",
            "nonsmooth = 0.008749\n",
            "smooth = 0.012686\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 400----------\n",
            "Loss = 0.021012\n",
            "nonsmooth = 0.008748\n",
            "smooth = 0.012265\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 500----------\n",
            "Loss = 0.020671\n",
            "nonsmooth = 0.008746\n",
            "smooth = 0.011925\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 600----------\n",
            "Loss = 0.020389\n",
            "nonsmooth = 0.008745\n",
            "smooth = 0.011644\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 700----------\n",
            "Loss = 0.020152\n",
            "nonsmooth = 0.008743\n",
            "smooth = 0.011409\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 800----------\n",
            "Loss = 0.019949\n",
            "nonsmooth = 0.008742\n",
            "smooth = 0.011207\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 900----------\n",
            "Loss = 0.019772\n",
            "nonsmooth = 0.008741\n",
            "smooth = 0.011031\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1000----------\n",
            "Loss = 0.019615\n",
            "nonsmooth = 0.008739\n",
            "smooth = 0.010876\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1100----------\n",
            "Loss = 0.019475\n",
            "nonsmooth = 0.008738\n",
            "smooth = 0.010737\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1200----------\n",
            "Loss = 0.019347\n",
            "nonsmooth = 0.008737\n",
            "smooth = 0.010611\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1300----------\n",
            "Loss = 0.019231\n",
            "nonsmooth = 0.008735\n",
            "smooth = 0.010496\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1400----------\n",
            "Loss = 0.019124\n",
            "nonsmooth = 0.008734\n",
            "smooth = 0.010390\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1500----------\n",
            "Loss = 0.019024\n",
            "nonsmooth = 0.008733\n",
            "smooth = 0.010292\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1600----------\n",
            "Loss = 0.018932\n",
            "nonsmooth = 0.008731\n",
            "smooth = 0.010201\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1700----------\n",
            "Loss = 0.018845\n",
            "nonsmooth = 0.008730\n",
            "smooth = 0.010115\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1800----------\n",
            "Loss = 0.018764\n",
            "nonsmooth = 0.008729\n",
            "smooth = 0.010035\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 1900----------\n",
            "Loss = 0.018687\n",
            "nonsmooth = 0.008727\n",
            "smooth = 0.009959\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2000----------\n",
            "Loss = 0.018614\n",
            "nonsmooth = 0.008726\n",
            "smooth = 0.009888\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2100----------\n",
            "Loss = 0.018545\n",
            "nonsmooth = 0.008725\n",
            "smooth = 0.009820\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2200----------\n",
            "Loss = 0.018480\n",
            "nonsmooth = 0.008723\n",
            "smooth = 0.009756\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2300----------\n",
            "Loss = 0.018417\n",
            "nonsmooth = 0.008722\n",
            "smooth = 0.009695\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2400----------\n",
            "Loss = 0.018358\n",
            "nonsmooth = 0.008721\n",
            "smooth = 0.009637\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2500----------\n",
            "Loss = 0.018301\n",
            "nonsmooth = 0.008719\n",
            "smooth = 0.009582\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2600----------\n",
            "Loss = 0.018247\n",
            "nonsmooth = 0.008718\n",
            "smooth = 0.009529\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2700----------\n",
            "Loss = 0.018195\n",
            "nonsmooth = 0.008717\n",
            "smooth = 0.009479\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2800----------\n",
            "Loss = 0.018146\n",
            "nonsmooth = 0.008716\n",
            "smooth = 0.009430\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 2900----------\n",
            "Loss = 0.018098\n",
            "nonsmooth = 0.008714\n",
            "smooth = 0.009384\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3000----------\n",
            "Loss = 0.018052\n",
            "nonsmooth = 0.008713\n",
            "smooth = 0.009339\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3100----------\n",
            "Loss = 0.018008\n",
            "nonsmooth = 0.008712\n",
            "smooth = 0.009296\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3200----------\n",
            "Loss = 0.017966\n",
            "nonsmooth = 0.008710\n",
            "smooth = 0.009255\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3300----------\n",
            "Loss = 0.017925\n",
            "nonsmooth = 0.008709\n",
            "smooth = 0.009216\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3400----------\n",
            "Loss = 0.017886\n",
            "nonsmooth = 0.008708\n",
            "smooth = 0.009178\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3500----------\n",
            "Loss = 0.017848\n",
            "nonsmooth = 0.008707\n",
            "smooth = 0.009141\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3600----------\n",
            "Loss = 0.017811\n",
            "nonsmooth = 0.008705\n",
            "smooth = 0.009105\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3700----------\n",
            "Loss = 0.017775\n",
            "nonsmooth = 0.008704\n",
            "smooth = 0.009071\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3800----------\n",
            "Loss = 0.017741\n",
            "nonsmooth = 0.008703\n",
            "smooth = 0.009038\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 3900----------\n",
            "Loss = 0.017708\n",
            "nonsmooth = 0.008702\n",
            "smooth = 0.009006\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 4000----------\n",
            "Loss = 0.017675\n",
            "nonsmooth = 0.008700\n",
            "smooth = 0.008975\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 4100----------\n",
            "Loss = 0.017644\n",
            "nonsmooth = 0.008699\n",
            "smooth = 0.008945\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 4200----------\n",
            "Loss = 0.017614\n",
            "nonsmooth = 0.008698\n",
            "smooth = 0.008916\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 4300----------\n",
            "Loss = 0.017584\n",
            "nonsmooth = 0.008697\n",
            "smooth = 0.008888\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 4400----------\n",
            "Loss = 0.017556\n",
            "nonsmooth = 0.008695\n",
            "smooth = 0.008860\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 4500----------\n",
            "Loss = 0.017528\n",
            "nonsmooth = 0.008694\n",
            "smooth = 0.008834\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 4600----------\n",
            "Loss = 0.017501\n",
            "nonsmooth = 0.008693\n",
            "smooth = 0.008808\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 4700----------\n",
            "Loss = 0.017475\n",
            "nonsmooth = 0.008692\n",
            "smooth = 0.008783\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 4800----------\n",
            "Loss = 0.017449\n",
            "nonsmooth = 0.008690\n",
            "smooth = 0.008759\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 4900----------\n",
            "Loss = 0.017424\n",
            "nonsmooth = 0.008689\n",
            "smooth = 0.008735\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 5000----------\n",
            "Loss = 0.017400\n",
            "nonsmooth = 0.008688\n",
            "smooth = 0.008712\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 5100----------\n",
            "Loss = 0.017376\n",
            "nonsmooth = 0.008687\n",
            "smooth = 0.008690\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 5200----------\n",
            "Loss = 0.017353\n",
            "nonsmooth = 0.008685\n",
            "smooth = 0.008668\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 5300----------\n",
            "Loss = 0.017331\n",
            "nonsmooth = 0.008684\n",
            "smooth = 0.008647\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 5400----------\n",
            "Loss = 0.017309\n",
            "nonsmooth = 0.008683\n",
            "smooth = 0.008626\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 5500----------\n",
            "Loss = 0.017288\n",
            "nonsmooth = 0.008682\n",
            "smooth = 0.008606\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 5600----------\n",
            "Loss = 0.017267\n",
            "nonsmooth = 0.008680\n",
            "smooth = 0.008586\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 5700----------\n",
            "Loss = 0.017246\n",
            "nonsmooth = 0.008679\n",
            "smooth = 0.008567\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 5800----------\n",
            "Loss = 0.017226\n",
            "nonsmooth = 0.008678\n",
            "smooth = 0.008548\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 5900----------\n",
            "Loss = 0.017207\n",
            "nonsmooth = 0.008677\n",
            "smooth = 0.008530\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 6000----------\n",
            "Loss = 0.017188\n",
            "nonsmooth = 0.008675\n",
            "smooth = 0.008512\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 6100----------\n",
            "Loss = 0.017169\n",
            "nonsmooth = 0.008674\n",
            "smooth = 0.008495\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 6200----------\n",
            "Loss = 0.017151\n",
            "nonsmooth = 0.008673\n",
            "smooth = 0.008478\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 6300----------\n",
            "Loss = 0.017133\n",
            "nonsmooth = 0.008672\n",
            "smooth = 0.008461\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 6400----------\n",
            "Loss = 0.017116\n",
            "nonsmooth = 0.008670\n",
            "smooth = 0.008445\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 6500----------\n",
            "Loss = 0.017098\n",
            "nonsmooth = 0.008669\n",
            "smooth = 0.008429\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 6600----------\n",
            "Loss = 0.017082\n",
            "nonsmooth = 0.008668\n",
            "smooth = 0.008414\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 6700----------\n",
            "Loss = 0.017065\n",
            "nonsmooth = 0.008667\n",
            "smooth = 0.008399\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 6800----------\n",
            "Loss = 0.017049\n",
            "nonsmooth = 0.008665\n",
            "smooth = 0.008384\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 6900----------\n",
            "Loss = 0.017033\n",
            "nonsmooth = 0.008664\n",
            "smooth = 0.008369\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 7000----------\n",
            "Loss = 0.017018\n",
            "nonsmooth = 0.008663\n",
            "smooth = 0.008355\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 7100----------\n",
            "Loss = 0.017003\n",
            "nonsmooth = 0.008662\n",
            "smooth = 0.008341\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 7200----------\n",
            "Loss = 0.016988\n",
            "nonsmooth = 0.008660\n",
            "smooth = 0.008327\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 7300----------\n",
            "Loss = 0.016973\n",
            "nonsmooth = 0.008659\n",
            "smooth = 0.008314\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 7400----------\n",
            "Loss = 0.016959\n",
            "nonsmooth = 0.008658\n",
            "smooth = 0.008301\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 7500----------\n",
            "Loss = 0.016945\n",
            "nonsmooth = 0.008657\n",
            "smooth = 0.008288\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 7600----------\n",
            "Loss = 0.016931\n",
            "nonsmooth = 0.008656\n",
            "smooth = 0.008276\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 7700----------\n",
            "Loss = 0.016918\n",
            "nonsmooth = 0.008654\n",
            "smooth = 0.008263\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 7800----------\n",
            "Loss = 0.016904\n",
            "nonsmooth = 0.008653\n",
            "smooth = 0.008251\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 7900----------\n",
            "Loss = 0.016891\n",
            "nonsmooth = 0.008652\n",
            "smooth = 0.008239\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 8000----------\n",
            "Loss = 0.016878\n",
            "nonsmooth = 0.008651\n",
            "smooth = 0.008228\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 8100----------\n",
            "Loss = 0.016866\n",
            "nonsmooth = 0.008649\n",
            "smooth = 0.008216\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 8200----------\n",
            "Loss = 0.016853\n",
            "nonsmooth = 0.008648\n",
            "smooth = 0.008205\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 8300----------\n",
            "Loss = 0.016841\n",
            "nonsmooth = 0.008647\n",
            "smooth = 0.008194\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 8400----------\n",
            "Loss = 0.016829\n",
            "nonsmooth = 0.008646\n",
            "smooth = 0.008183\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 8500----------\n",
            "Loss = 0.016817\n",
            "nonsmooth = 0.008644\n",
            "smooth = 0.008173\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 8600----------\n",
            "Loss = 0.016806\n",
            "nonsmooth = 0.008643\n",
            "smooth = 0.008162\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 8700----------\n",
            "Loss = 0.016794\n",
            "nonsmooth = 0.008642\n",
            "smooth = 0.008152\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 8800----------\n",
            "Loss = 0.016783\n",
            "nonsmooth = 0.008641\n",
            "smooth = 0.008142\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 8900----------\n",
            "Loss = 0.016772\n",
            "nonsmooth = 0.008639\n",
            "smooth = 0.008132\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 9000----------\n",
            "Loss = 0.016761\n",
            "nonsmooth = 0.008638\n",
            "smooth = 0.008123\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 9100----------\n",
            "Loss = 0.016750\n",
            "nonsmooth = 0.008637\n",
            "smooth = 0.008113\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 9200----------\n",
            "Loss = 0.016740\n",
            "nonsmooth = 0.008636\n",
            "smooth = 0.008104\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 9300----------\n",
            "Loss = 0.016729\n",
            "nonsmooth = 0.008634\n",
            "smooth = 0.008095\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 9400----------\n",
            "Loss = 0.016719\n",
            "nonsmooth = 0.008633\n",
            "smooth = 0.008086\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 9500----------\n",
            "Loss = 0.016709\n",
            "nonsmooth = 0.008632\n",
            "smooth = 0.008077\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 9600----------\n",
            "Loss = 0.016699\n",
            "nonsmooth = 0.008631\n",
            "smooth = 0.008068\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 9700----------\n",
            "Loss = 0.016689\n",
            "nonsmooth = 0.008629\n",
            "smooth = 0.008060\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 9800----------\n",
            "Loss = 0.016680\n",
            "nonsmooth = 0.008628\n",
            "smooth = 0.008051\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 9900----------\n",
            "Loss = 0.016670\n",
            "nonsmooth = 0.008627\n",
            "smooth = 0.008043\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 10000----------\n",
            "Loss = 0.016661\n",
            "nonsmooth = 0.008626\n",
            "smooth = 0.008035\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 10100----------\n",
            "Loss = 0.016652\n",
            "nonsmooth = 0.008625\n",
            "smooth = 0.008027\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 10200----------\n",
            "Loss = 0.016642\n",
            "nonsmooth = 0.008623\n",
            "smooth = 0.008019\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 10300----------\n",
            "Loss = 0.016633\n",
            "nonsmooth = 0.008622\n",
            "smooth = 0.008011\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 10400----------\n",
            "Loss = 0.016625\n",
            "nonsmooth = 0.008621\n",
            "smooth = 0.008004\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 10500----------\n",
            "Loss = 0.016616\n",
            "nonsmooth = 0.008620\n",
            "smooth = 0.007996\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 10600----------\n",
            "Loss = 0.016607\n",
            "nonsmooth = 0.008618\n",
            "smooth = 0.007989\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 10700----------\n",
            "Loss = 0.016599\n",
            "nonsmooth = 0.008617\n",
            "smooth = 0.007982\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 10800----------\n",
            "Loss = 0.016590\n",
            "nonsmooth = 0.008616\n",
            "smooth = 0.007975\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 10900----------\n",
            "Loss = 0.016582\n",
            "nonsmooth = 0.008615\n",
            "smooth = 0.007968\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 11000----------\n",
            "Loss = 0.016574\n",
            "nonsmooth = 0.008613\n",
            "smooth = 0.007961\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 11100----------\n",
            "Loss = 0.016566\n",
            "nonsmooth = 0.008612\n",
            "smooth = 0.007954\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 11200----------\n",
            "Loss = 0.016558\n",
            "nonsmooth = 0.008611\n",
            "smooth = 0.007947\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 11300----------\n",
            "Loss = 0.016550\n",
            "nonsmooth = 0.008610\n",
            "smooth = 0.007941\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 11400----------\n",
            "Loss = 0.016542\n",
            "nonsmooth = 0.008608\n",
            "smooth = 0.007934\n",
            "Variable usage = 100.00%\n",
            "----------Iter = 11500----------\n",
            "Loss = 0.016535\n",
            "nonsmooth = 0.008607\n",
            "smooth = 0.007928\n",
            "Variable usage = 100.00%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6b961175e50a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Train with ISTA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   train_loss_list = train_model_ista(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mcdlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0001\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.0001\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam_ridge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'H'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     check_every=100)\n",
            "\u001b[0;32m/content/ngcausality/models/cdlinear.py\u001b[0m in \u001b[0;36mtrain_model_ista\u001b[0;34m(cdlinear, X, lr, max_iter, lam, lam_ridge, penalty, lookback, check_every, verbose)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mprox_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mcdlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# Calculate loss for next iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m   2908\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2909\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mset_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2910\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2911\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2912\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i in range(11):\n",
        "  save_dir = '/content/drive/MyDrive/ngcausality_results/' + 'cdlinear_var2_t250/' + str(0.0001 + 0.0001 * i) + '/'\n",
        "\n",
        "  if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "  #Set up model\n",
        "  cdlinear = cDLinear(X.shape[-1], hidden=100, lag=3).to(device=device)\n",
        "\n",
        "  # Train with ISTA\n",
        "  train_loss_list = train_model_ista(\n",
        "    cdlinear, X, lam=(0.0001 + 0.0001 * i), lam_ridge=1e-2, lr=1e-1, penalty='H', max_iter=50000,\n",
        "    check_every=100)\n",
        "\n",
        "  # Loss function plot\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  train_loss_np = [loss.cpu().detach().numpy() for loss in train_loss_list]\n",
        "  plt.plot(50 * np.arange(len(train_loss_np)), train_loss_np)\n",
        "  plt.title('cDLinear training')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Training steps')\n",
        "  plt.tight_layout()\n",
        "  loss_plot_path = os.path.join(save_dir, f'loss_plot_{0.0001 + 0.0001 * i}.png')\n",
        "  plt.savefig(loss_plot_path)  # Save the loss plot to Google Drive\n",
        "  plt.close()  # Close the plot to prevent it from displaying\n",
        "\n",
        "  # Verify learned Granger causality\n",
        "  GC_est = cdlinear.GC().cpu().data.numpy()\n",
        "\n",
        "  results_file_path = os.path.join(save_dir, f'gc_results_{0.0001 + 0.0001 * i}.txt')\n",
        "  with open(results_file_path, 'w') as f:\n",
        "    f.write(f'True variable usage = {100 * np.mean(GC)}%\\n')\n",
        "    f.write(f'Estimated variable usage = {100 * np.mean(GC_est)}%\\n')\n",
        "    f.write(f'Accuracy = {100 * np.mean(GC == GC_est)}%\\n')\n",
        "    f.write(f'True positives = {np.sum((GC == 1) & (GC_est == 1))}\\n')\n",
        "    f.write(f'True negatives = {np.sum((GC == 0) & (GC_est == 0))}\\n')\n",
        "    f.write(f'False positives = {np.sum((GC == 0) & (GC_est == 1))}\\n')\n",
        "    f.write(f'False negatives = {np.sum((GC == 1) & (GC_est == 0))}\\n')\n",
        "\n",
        "  # Make figures for Granger causality matrices\n",
        "  fig, axarr = plt.subplots(1, 2, figsize=(16, 5))\n",
        "  axarr[0].imshow(GC, cmap='Blues')\n",
        "  axarr[0].set_title('GC actual')\n",
        "  axarr[0].set_ylabel('Affected series')\n",
        "  axarr[0].set_xlabel('Causal series')\n",
        "  axarr[0].set_xticks([])\n",
        "  axarr[0].set_yticks([])\n",
        "\n",
        "  axarr[1].imshow(GC_est, cmap='Blues', vmin=0, vmax=1, extent=(0, len(GC_est), len(GC_est), 0))\n",
        "  axarr[1].set_title('GC estimated')\n",
        "  axarr[1].set_ylabel('Affected series')\n",
        "  axarr[1].set_xlabel('Causal series')\n",
        "  axarr[1].set_xticks([])\n",
        "  axarr[1].set_yticks([])\n",
        "\n",
        "  # Mark disagreements\n",
        "  for i in range(len(GC_est)):\n",
        "    for j in range(len(GC_est)):\n",
        "        if GC[i, j] != GC_est[i, j]:\n",
        "            rect = plt.Rectangle((j, i-0.05), 1, 1, facecolor='none', edgecolor='red', linewidth=1)\n",
        "            axarr[1].add_patch(rect)\n",
        "\n",
        "  gc_plot_path = os.path.join(save_dir, f'gc_plot_{0.0001 + 0.0001 * i}.png')\n",
        "  plt.savefig(gc_plot_path)  # Save the GC plot to Google Drive\n",
        "  plt.close()  # Close the plot to prevent it from displaying\n",
        "\n",
        "  # Verify lag selection\n",
        "  for i in range(len(GC_est)):\n",
        "    # Get true GC\n",
        "    GC_lag = np.zeros((5, len(GC_est)))\n",
        "    GC_lag[:3, GC[i].astype(bool)] = 1.0\n",
        "\n",
        "    # Get estimated GC\n",
        "    GC_est_lag = cdlinear.GC(ignore_lag=False, threshold=False)[i].cpu().data.numpy().T[::-1]\n",
        "\n",
        "    # Make figures for lag-based GC\n",
        "    fig, axarr = plt.subplots(1, 2, figsize=(16, 5))\n",
        "    axarr[0].imshow(GC_lag, cmap='Blues', extent=(0, len(GC_est), 5, 0))\n",
        "    axarr[0].set_title(f'Series {i + 1} true GC')\n",
        "    axarr[0].set_ylabel('Lag')\n",
        "    axarr[0].set_xlabel('Series')\n",
        "    axarr[0].set_xticks(np.arange(len(GC_est)) + 0.5)\n",
        "    axarr[0].set_xticklabels(range(len(GC_est)))\n",
        "    axarr[0].set_yticks(np.arange(5) + 0.5)\n",
        "    axarr[0].set_yticklabels(range(1, 5 + 1))\n",
        "    axarr[0].tick_params(axis='both', length=0)\n",
        "\n",
        "    axarr[1].imshow(GC_est_lag, cmap='Blues', extent=(0, len(GC_est), 5, 0))\n",
        "    axarr[1].set_title(f'Series {i + 1} estimated GC')\n",
        "    axarr[1].set_ylabel('Lag')\n",
        "    axarr[1].set_xlabel('Series')\n",
        "    axarr[1].set_xticks(np.arange(len(GC_est)) + 0.5)\n",
        "    axarr[1].set_xticklabels(range(len(GC_est)))\n",
        "    axarr[1].set_yticks(np.arange(5) + 0.5)\n",
        "    axarr[1].set_yticklabels(range(1, 5 + 1))\n",
        "    axarr[1].tick_params(axis='both', length=0)\n",
        "\n",
        "    # Mark nonzeros\n",
        "    for k in range(len(GC_est)):\n",
        "        for j in range(5):\n",
        "            if GC_est_lag[j, k] > 0.0:\n",
        "                rect = plt.Rectangle((k, j), 1, 1, facecolor='none', edgecolor='green', linewidth=1.0)\n",
        "                axarr[1].add_patch(rect)\n",
        "\n",
        "    lag_gc_plot_path = os.path.join(save_dir, f'lag_gc_plot_{j}_{i}.png')\n",
        "    plt.savefig(lag_gc_plot_path)  # Save lag GC plot to Google Drive\n",
        "    plt.close()  # Close the plot to prevent it from displaying\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}